{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load src/\n",
    "import sys\n",
    "import random\n",
    "sys.path.append('../')\n",
    "import src.dataset\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = src.dataset.load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For final submission:\n",
    "#import scipy\n",
    "#X_train = X_train + X_valid\n",
    "#mean = X_train[X_train.nonzero()].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 12 15:53:39 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 410.104      Driver Version: 410.104      CUDA Version: 10.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    On   | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   36C    P8    15W / 230W |     42MiB /  8117MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1625      G   /usr/lib/xorg/Xorg                            31MiB |\r\n",
      "|    0      1818      G   /usr/bin/gnome-shell                           6MiB |\r\n",
      "|    0      2413      G   /opt/teamviewer/tv_bin/TeamViewer              2MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.count_nonzero(), X_valid.count_nonzero(), X_test.count_nonzero()\n",
    "valid_indices = list(set(zip(X_train.nonzero()[0], X_train.nonzero()[1])))\n",
    "# valid_per_line = [[] for _ in range(10000)]\n",
    "# for i, j in valid_indices:\n",
    "#     valid_per_line[i].append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# def count_vals(X, valid_indices):\n",
    "#     counts = {}\n",
    "#     for i, j in valid_indices:\n",
    "#         val = X[i, j]\n",
    "#         counts.setdefault(val, 0)\n",
    "#         counts[val] += 1\n",
    "#     return counts\n",
    "# print(count_vals(X_train, valid_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# X_unbias = X_train.copy()\n",
    "# for i, j in valid_indices:\n",
    "#     val = X_train[i, j]\n",
    "#     if val >= 3 and random.random() < 0.5:\n",
    "#         X_unbias[i, j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_indices_unbias = list(set(zip(X_unbias.nonzero()[0], X_unbias.nonzero()[1])))\n",
    "# print(count_vals(X_unbias, valid_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_regularization(model):\n",
    "    l2_reg = None\n",
    "    for W in model.parameters():\n",
    "        if l2_reg is None:\n",
    "            l2_reg = W.norm(2)\n",
    "        else:\n",
    "            l2_reg = l2_reg + W.norm(2)\n",
    "    return l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_dim = 10000\n",
    "        self.item_dim = 1000\n",
    "        self.embed_dim = 150\n",
    "        self.user_embed = nn.Embedding(self.user_dim, self.embed_dim)\n",
    "        self.item_embed = nn.Embedding(self.item_dim, self.embed_dim)\n",
    "        self.layer_1 = nn.Linear(self.embed_dim*2, 300)\n",
    "        self.layer_1a = nn.Linear(300, 300)\n",
    "        self.layer_2 = nn.Linear(300, 50)\n",
    "        self.layer_3 = nn.Linear(50, 50)\n",
    "        self.layer_3a = nn.Linear(50, 300)\n",
    "        \n",
    "        self.cls_layer = nn.Linear(300, 1)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        user_idx, item_idx = torch.split(data, 1, dim=1)\n",
    "#         print(\"user\",user_idx)\n",
    "#         print(\"item\",item_idx)\n",
    "        user_idx = torch.squeeze(user_idx, dim=-1)\n",
    "        item_idx = torch.squeeze(item_idx, dim=-1)\n",
    "\n",
    "        user_embedding = self.user_embed(user_idx)\n",
    "        item_embedding = self.item_embed(item_idx)\n",
    "        assert user_embedding.shape[-1] == item_embedding.shape[-1] == self.embed_dim\n",
    "\n",
    "        # Input is concatenation\n",
    "        net_data = torch.cat([user_embedding, item_embedding], dim=-1)\n",
    "\n",
    "        # Feedforward layers\n",
    "        net_data = F.relu(self.layer_1(net_data))\n",
    "        net_data = F.dropout(net_data)\n",
    "        \n",
    "        net_data = F.relu(self.layer_1a(net_data))\n",
    "        net_data = F.dropout(net_data)\n",
    "        \n",
    "        net_data = F.relu(self.layer_2(net_data))\n",
    "        net_data = F.dropout(net_data)\n",
    "        \n",
    "        net_data = F.relu(self.layer_3(net_data))\n",
    "        net_data = F.dropout(net_data)\n",
    "        \n",
    "        net_data = F.relu(self.layer_3a(net_data))\n",
    "        net_data = F.dropout(net_data)\n",
    "        \n",
    "        # Predict score for position A_i_j\n",
    "        y_score = self.cls_layer(net_data)\n",
    "#         y_score = (4.0 * torch.sigmoid(y_score)) + 1.0\n",
    "#         return torch.sigmoid(y_score) * 6.0\n",
    "        return y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "model = Autoencoder().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925567 251385\n"
     ]
    }
   ],
   "source": [
    "print(X_train.count_nonzero(), X_valid.count_nonzero())\n",
    "\n",
    "def get_predictions(A):\n",
    "    model.eval()\n",
    "    A_pred = np.zeros((10000, 1000))\n",
    "    \n",
    "    valid_indices = list(set(zip(A.nonzero()[0], A.nonzero()[1])))\n",
    "    for i in range(0, len(valid_indices), 64):\n",
    "        X = valid_indices[i:i+64]\n",
    "        X_ = torch.tensor(X, dtype=torch.long, device=device)\n",
    "        y_preds = model(X_)\n",
    "        if USE_CLS:\n",
    "            y_preds = torch.argmax(torch.softmax(y_preds, dim=-1), dim=-1)\n",
    "            if i == 0:\n",
    "                print(y_preds)\n",
    "        for j in range(len(X)):\n",
    "            pred = y_preds[j]\n",
    "            A_pred[X[j]] = pred\n",
    "\n",
    "    print(\"Stats:\", np.mean(A_pred[A_pred.nonzero()]), np.std(A_pred.nonzero()))\n",
    "    return A_pred, valid_indices\n",
    "\n",
    "def compute_loss():\n",
    "    A_pred, valid_indices = get_predictions(X_valid)\n",
    "    losses = np.square(X_valid - A_pred)\n",
    "    losses = [losses[i,j] for (i, j) in valid_indices]\n",
    "    mean_loss = np.mean(losses)\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 0 Avg loss: 15.8741\n",
      "It 1000 Avg loss: 2.1456\n",
      "It 2000 Avg loss: 1.8474\n",
      "It 3000 Avg loss: 1.6664\n",
      "It 4000 Avg loss: 1.5339\n",
      "It 5000 Avg loss: 1.4134\n",
      "It 6000 Avg loss: 1.3581\n",
      "It 7000 Avg loss: 1.3275\n",
      "It 8000 Avg loss: 1.2856\n",
      "It 9000 Avg loss: 1.2638\n",
      "It 10000 Avg loss: 1.2848\n",
      "Stats: 3.8287998216420642 3104.080220526129\n",
      "Mean loss: 1.2716\n",
      "It 11000 Avg loss: 1.2687\n",
      "It 12000 Avg loss: 1.2612\n",
      "It 13000 Avg loss: 1.2521\n",
      "It 14000 Avg loss: 1.2481\n",
      "It 15000 Avg loss: 1.2474\n",
      "It 16000 Avg loss: 1.2156\n",
      "It 17000 Avg loss: 1.2348\n",
      "It 18000 Avg loss: 1.2217\n",
      "It 19000 Avg loss: 1.2345\n",
      "It 20000 Avg loss: 1.2139\n",
      "Stats: 3.8869121625784446 3104.080220526129\n",
      "Mean loss: 1.2086\n",
      "It 21000 Avg loss: 1.2058\n",
      "It 22000 Avg loss: 1.2215\n",
      "It 23000 Avg loss: 1.1908\n",
      "It 24000 Avg loss: 1.1703\n",
      "It 25000 Avg loss: 1.1876\n",
      "It 26000 Avg loss: 1.1439\n",
      "It 27000 Avg loss: 1.2039\n",
      "It 28000 Avg loss: 1.1833\n",
      "It 29000 Avg loss: 1.1759\n",
      "It 30000 Avg loss: 1.1699\n",
      "Stats: 3.8443906837586446 3104.080220526129\n",
      "Mean loss: 1.1669\n",
      "It 31000 Avg loss: 1.1553\n",
      "It 32000 Avg loss: 1.1929\n",
      "It 33000 Avg loss: 1.1557\n",
      "It 34000 Avg loss: 1.1795\n",
      "It 35000 Avg loss: 1.1452\n",
      "It 36000 Avg loss: 1.1531\n",
      "It 37000 Avg loss: 1.1466\n",
      "It 38000 Avg loss: 1.1388\n",
      "It 39000 Avg loss: 1.1385\n",
      "It 40000 Avg loss: 1.1091\n",
      "Stats: 3.8790451746542525 3104.080220526129\n",
      "Mean loss: 1.1498\n",
      "It 41000 Avg loss: 1.1522\n",
      "It 42000 Avg loss: 1.1400\n",
      "It 43000 Avg loss: 1.1351\n",
      "It 44000 Avg loss: 1.1092\n",
      "It 45000 Avg loss: 1.1170\n",
      "It 46000 Avg loss: 1.1371\n",
      "It 47000 Avg loss: 1.1302\n",
      "It 48000 Avg loss: 1.1318\n",
      "It 49000 Avg loss: 1.1332\n",
      "It 50000 Avg loss: 1.1304\n",
      "Stats: 3.8672823012428794 3104.080220526129\n",
      "Mean loss: 1.1354\n",
      "It 51000 Avg loss: 1.1222\n",
      "It 52000 Avg loss: 1.1068\n",
      "It 53000 Avg loss: 1.1350\n",
      "It 54000 Avg loss: 1.1155\n",
      "It 55000 Avg loss: 1.1231\n",
      "It 56000 Avg loss: 1.0993\n",
      "It 57000 Avg loss: 1.1250\n",
      "It 58000 Avg loss: 1.1355\n",
      "It 59000 Avg loss: 1.1188\n",
      "It 60000 Avg loss: 1.1218\n",
      "Stats: 3.869510006968146 3104.080220526129\n",
      "Mean loss: 1.1321\n",
      "It 61000 Avg loss: 1.1239\n",
      "It 62000 Avg loss: 1.1099\n",
      "It 63000 Avg loss: 1.1126\n",
      "It 64000 Avg loss: 1.0996\n",
      "It 65000 Avg loss: 1.1242\n",
      "It 66000 Avg loss: 1.0959\n",
      "It 67000 Avg loss: 1.1081\n",
      "It 68000 Avg loss: 1.0904\n",
      "It 69000 Avg loss: 1.1080\n",
      "It 70000 Avg loss: 1.1130\n",
      "Stats: 3.8598546173828754 3104.080220526129\n",
      "Mean loss: 1.1166\n",
      "It 71000 Avg loss: 1.0870\n",
      "It 72000 Avg loss: 1.0927\n",
      "It 73000 Avg loss: 1.0984\n",
      "It 74000 Avg loss: 1.1066\n",
      "It 75000 Avg loss: 1.0816\n",
      "It 76000 Avg loss: 1.1058\n",
      "It 77000 Avg loss: 1.0973\n",
      "It 78000 Avg loss: 1.1139\n",
      "It 79000 Avg loss: 1.0996\n",
      "It 80000 Avg loss: 1.0984\n",
      "Stats: 3.843127378971757 3104.080220526129\n",
      "Mean loss: 1.1095\n",
      "It 81000 Avg loss: 1.0807\n",
      "It 82000 Avg loss: 1.0898\n",
      "It 83000 Avg loss: 1.0969\n",
      "It 84000 Avg loss: 1.1090\n",
      "It 85000 Avg loss: 1.0824\n",
      "It 86000 Avg loss: 1.1090\n",
      "It 87000 Avg loss: 1.0951\n",
      "It 88000 Avg loss: 1.0874\n",
      "It 89000 Avg loss: 1.0872\n",
      "It 90000 Avg loss: 1.0881\n",
      "Stats: 3.8179771046162667 3104.080220526129\n",
      "Mean loss: 1.1041\n",
      "It 91000 Avg loss: 1.0892\n",
      "It 92000 Avg loss: 1.1003\n",
      "It 93000 Avg loss: 1.1098\n",
      "It 94000 Avg loss: 1.0892\n",
      "It 95000 Avg loss: 1.0965\n",
      "It 96000 Avg loss: 1.0969\n",
      "It 97000 Avg loss: 1.0840\n",
      "It 98000 Avg loss: 1.0974\n",
      "It 99000 Avg loss: 1.1065\n",
      "It 100000 Avg loss: 1.1026\n",
      "Stats: 3.8150952992320812 3104.080220526129\n",
      "Mean loss: 1.0997\n",
      "It 101000 Avg loss: 1.0818\n",
      "It 102000 Avg loss: 1.0818\n",
      "It 103000 Avg loss: 1.0698\n",
      "It 104000 Avg loss: 1.0826\n",
      "It 105000 Avg loss: 1.0965\n",
      "It 106000 Avg loss: 1.0859\n",
      "It 107000 Avg loss: 1.0702\n",
      "It 108000 Avg loss: 1.0804\n",
      "It 109000 Avg loss: 1.0843\n",
      "It 110000 Avg loss: 1.0658\n",
      "Stats: 3.840652623567229 3104.080220526129\n",
      "Mean loss: 1.0924\n",
      "It 111000 Avg loss: 1.0598\n",
      "It 112000 Avg loss: 1.0741\n",
      "It 113000 Avg loss: 1.0779\n",
      "It 114000 Avg loss: 1.0575\n",
      "It 115000 Avg loss: 1.0567\n",
      "It 116000 Avg loss: 1.0964\n",
      "It 117000 Avg loss: 1.0782\n",
      "It 118000 Avg loss: 1.0668\n",
      "It 119000 Avg loss: 1.0748\n",
      "It 120000 Avg loss: 1.0900\n",
      "Stats: 3.8577726653783233 3104.080220526129\n",
      "Mean loss: 1.0874\n",
      "It 121000 Avg loss: 1.0881\n",
      "It 122000 Avg loss: 1.0764\n",
      "It 123000 Avg loss: 1.0770\n",
      "It 124000 Avg loss: 1.0865\n",
      "It 125000 Avg loss: 1.0773\n",
      "It 126000 Avg loss: 1.0559\n",
      "It 127000 Avg loss: 1.0613\n",
      "It 128000 Avg loss: 1.0584\n",
      "It 129000 Avg loss: 1.0597\n",
      "It 130000 Avg loss: 1.0409\n",
      "Stats: 3.829585444605938 3104.080220526129\n",
      "Mean loss: 1.0836\n",
      "It 131000 Avg loss: 1.0516\n",
      "It 132000 Avg loss: 1.0509\n",
      "It 133000 Avg loss: 1.0640\n",
      "It 134000 Avg loss: 1.0496\n",
      "It 135000 Avg loss: 1.0705\n",
      "It 136000 Avg loss: 1.0663\n",
      "It 137000 Avg loss: 1.0665\n",
      "It 138000 Avg loss: 1.0576\n",
      "It 139000 Avg loss: 1.0892\n",
      "It 140000 Avg loss: 1.0642\n",
      "Stats: 3.847741798815661 3104.080220526129\n",
      "Mean loss: 1.0788\n",
      "It 141000 Avg loss: 1.0589\n",
      "It 142000 Avg loss: 1.0602\n",
      "It 143000 Avg loss: 1.0491\n",
      "It 144000 Avg loss: 1.0479\n",
      "It 145000 Avg loss: 1.0585\n",
      "It 146000 Avg loss: 1.0533\n",
      "It 147000 Avg loss: 1.0391\n",
      "It 148000 Avg loss: 1.0369\n",
      "It 149000 Avg loss: 1.0561\n",
      "It 150000 Avg loss: 1.0463\n",
      "Stats: 3.834085733154512 3104.080220526129\n",
      "Mean loss: 1.0735\n",
      "It 151000 Avg loss: 1.0645\n",
      "It 152000 Avg loss: 1.0542\n",
      "It 153000 Avg loss: 1.0674\n",
      "It 154000 Avg loss: 1.0467\n",
      "It 155000 Avg loss: 1.0719\n",
      "It 156000 Avg loss: 1.0350\n",
      "It 157000 Avg loss: 1.0529\n",
      "It 158000 Avg loss: 1.0459\n",
      "It 159000 Avg loss: 1.0397\n",
      "It 160000 Avg loss: 1.0272\n",
      "Stats: 3.878369632281696 3104.080220526129\n",
      "Mean loss: 1.0739\n",
      "It 161000 Avg loss: 1.0666\n",
      "It 162000 Avg loss: 1.0444\n",
      "It 163000 Avg loss: 1.0628\n",
      "It 164000 Avg loss: 1.0343\n",
      "It 165000 Avg loss: 1.0567\n",
      "It 166000 Avg loss: 1.0564\n",
      "It 167000 Avg loss: 1.0402\n",
      "It 168000 Avg loss: 1.0380\n",
      "It 169000 Avg loss: 1.0633\n",
      "It 170000 Avg loss: 1.0417\n",
      "Stats: 3.8202461919473083 3104.080220526129\n",
      "Mean loss: 1.0693\n",
      "It 171000 Avg loss: 1.0341\n",
      "It 172000 Avg loss: 1.0433\n",
      "It 173000 Avg loss: 1.0520\n",
      "It 174000 Avg loss: 1.0354\n",
      "It 175000 Avg loss: 1.0353\n",
      "It 176000 Avg loss: 1.0273\n",
      "It 177000 Avg loss: 1.0671\n",
      "It 178000 Avg loss: 1.0313\n",
      "It 179000 Avg loss: 1.0306\n",
      "It 180000 Avg loss: 1.0503\n",
      "Stats: 3.828407628219405 3104.080220526129\n",
      "Mean loss: 1.0692\n",
      "It 181000 Avg loss: 1.0369\n",
      "It 182000 Avg loss: 1.0230\n",
      "It 183000 Avg loss: 1.0419\n",
      "It 184000 Avg loss: 1.0358\n",
      "It 185000 Avg loss: 1.0585\n",
      "It 186000 Avg loss: 1.0365\n",
      "It 187000 Avg loss: 1.0241\n",
      "It 188000 Avg loss: 1.0413\n",
      "It 189000 Avg loss: 1.0422\n",
      "It 190000 Avg loss: 1.0428\n",
      "Stats: 3.853714815208398 3104.080220526129\n",
      "Mean loss: 1.0628\n",
      "It 191000 Avg loss: 1.0353\n",
      "It 192000 Avg loss: 1.0515\n",
      "It 193000 Avg loss: 1.0397\n",
      "It 194000 Avg loss: 1.0422\n",
      "It 195000 Avg loss: 1.0471\n",
      "It 196000 Avg loss: 1.0440\n",
      "It 197000 Avg loss: 1.0235\n",
      "It 198000 Avg loss: 1.0321\n",
      "It 199000 Avg loss: 1.0184\n",
      "It 200000 Avg loss: 1.0232\n",
      "Stats: 3.8615064381950854 3104.080220526129\n",
      "Mean loss: 1.0647\n",
      "It 201000 Avg loss: 1.0438\n",
      "It 202000 Avg loss: 1.0105\n",
      "It 203000 Avg loss: 1.0524\n",
      "It 204000 Avg loss: 1.0191\n",
      "It 205000 Avg loss: 1.0264\n",
      "It 206000 Avg loss: 1.0380\n",
      "It 207000 Avg loss: 1.0318\n",
      "It 208000 Avg loss: 1.0196\n",
      "It 209000 Avg loss: 1.0367\n",
      "It 210000 Avg loss: 1.0182\n",
      "Stats: 3.844241287553832 3104.080220526129\n",
      "Mean loss: 1.0608\n",
      "It 211000 Avg loss: 1.0282\n",
      "It 212000 Avg loss: 1.0163\n",
      "It 213000 Avg loss: 1.0301\n",
      "It 214000 Avg loss: 1.0545\n",
      "It 215000 Avg loss: 1.0096\n",
      "It 216000 Avg loss: 1.0208\n",
      "It 217000 Avg loss: 1.0438\n",
      "It 218000 Avg loss: 1.0264\n",
      "It 219000 Avg loss: 1.0324\n",
      "It 220000 Avg loss: 1.0328\n",
      "Stats: 3.8595556085494818 3104.080220526129\n",
      "Mean loss: 1.0571\n",
      "It 221000 Avg loss: 1.0317\n",
      "It 222000 Avg loss: 1.0278\n",
      "It 223000 Avg loss: 1.0049\n",
      "It 224000 Avg loss: 1.0189\n",
      "It 225000 Avg loss: 1.0204\n",
      "It 226000 Avg loss: 1.0382\n",
      "It 227000 Avg loss: 1.0239\n",
      "It 228000 Avg loss: 1.0269\n",
      "It 229000 Avg loss: 1.0246\n",
      "It 230000 Avg loss: 1.0193\n",
      "Stats: 3.8508516511586683 3104.080220526129\n",
      "Mean loss: 1.0549\n",
      "It 231000 Avg loss: 1.0260\n",
      "It 232000 Avg loss: 1.0164\n",
      "It 233000 Avg loss: 1.0040\n",
      "It 234000 Avg loss: 1.0105\n",
      "It 235000 Avg loss: 0.9973\n",
      "It 236000 Avg loss: 1.0319\n",
      "It 237000 Avg loss: 1.0052\n",
      "It 238000 Avg loss: 1.0491\n",
      "It 239000 Avg loss: 1.0056\n",
      "It 240000 Avg loss: 1.0142\n",
      "Stats: 3.837113457768198 3104.080220526129\n",
      "Mean loss: 1.0543\n",
      "It 241000 Avg loss: 1.0099\n",
      "It 242000 Avg loss: 1.0149\n",
      "It 243000 Avg loss: 1.0253\n",
      "It 244000 Avg loss: 1.0150\n",
      "It 245000 Avg loss: 1.0336\n",
      "It 246000 Avg loss: 1.0120\n",
      "It 247000 Avg loss: 1.0207\n",
      "It 248000 Avg loss: 1.0142\n",
      "It 249000 Avg loss: 1.0097\n",
      "It 250000 Avg loss: 1.0092\n",
      "Stats: 3.846654948636347 3104.080220526129\n",
      "Mean loss: 1.0527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 251000 Avg loss: 1.0315\n",
      "It 252000 Avg loss: 1.0174\n",
      "It 253000 Avg loss: 1.0075\n",
      "It 254000 Avg loss: 1.0264\n",
      "It 255000 Avg loss: 1.0119\n",
      "It 256000 Avg loss: 1.0088\n",
      "It 257000 Avg loss: 1.0182\n",
      "It 258000 Avg loss: 1.0074\n",
      "It 259000 Avg loss: 1.0077\n",
      "It 260000 Avg loss: 1.0131\n",
      "Stats: 3.8531360406386415 3104.080220526129\n",
      "Mean loss: 1.0520\n",
      "It 261000 Avg loss: 1.0264\n",
      "It 262000 Avg loss: 1.0151\n",
      "It 263000 Avg loss: 1.0157\n",
      "It 264000 Avg loss: 1.0170\n",
      "It 265000 Avg loss: 1.0165\n",
      "It 266000 Avg loss: 1.0147\n",
      "It 267000 Avg loss: 0.9997\n",
      "It 268000 Avg loss: 1.0139\n",
      "It 269000 Avg loss: 1.0222\n",
      "It 270000 Avg loss: 1.0040\n",
      "Stats: 3.854191223211262 3104.080220526129\n",
      "Mean loss: 1.0477\n",
      "It 271000 Avg loss: 1.0090\n",
      "It 272000 Avg loss: 1.0162\n",
      "It 273000 Avg loss: 1.0234\n",
      "It 274000 Avg loss: 1.0331\n",
      "It 275000 Avg loss: 0.9985\n",
      "It 276000 Avg loss: 0.9918\n",
      "It 277000 Avg loss: 1.0063\n",
      "It 278000 Avg loss: 1.0227\n",
      "It 279000 Avg loss: 1.0225\n",
      "It 280000 Avg loss: 1.0128\n",
      "Stats: 3.8903227723901224 3104.080220526129\n",
      "Mean loss: 1.0492\n",
      "It 281000 Avg loss: 0.9973\n",
      "It 282000 Avg loss: 1.0202\n",
      "It 283000 Avg loss: 1.0033\n",
      "It 284000 Avg loss: 1.0092\n",
      "It 285000 Avg loss: 0.9870\n",
      "It 286000 Avg loss: 1.0029\n",
      "It 287000 Avg loss: 1.0232\n",
      "It 288000 Avg loss: 1.0164\n",
      "It 289000 Avg loss: 1.0141\n",
      "It 290000 Avg loss: 1.0151\n",
      "Stats: 3.8219139652932026 3104.080220526129\n",
      "Mean loss: 1.0459\n",
      "It 291000 Avg loss: 1.0026\n",
      "It 292000 Avg loss: 1.0115\n",
      "It 293000 Avg loss: 1.0081\n",
      "It 294000 Avg loss: 1.0098\n",
      "It 295000 Avg loss: 1.0162\n",
      "It 296000 Avg loss: 1.0078\n",
      "It 297000 Avg loss: 1.0125\n",
      "It 298000 Avg loss: 1.0247\n",
      "It 299000 Avg loss: 0.9862\n"
     ]
    }
   ],
   "source": [
    "loss_avg = []\n",
    "USE_CLS = False\n",
    "\n",
    "for i in range(0, 300000):\n",
    "    # randomly sample 32 points from training matrix\n",
    "    model.train()\n",
    "    X = random.sample(valid_indices, k=32) # + [(random.randint(0, 9999), random.randint(0, 999)) for _ in range(8)]\n",
    "#     line = random.randint(0, 9999)\n",
    "#     valid__ = valid_per_line[line]\n",
    "#     X = valid__\n",
    "\n",
    "    y_true = [[X_train[Xi]] for Xi in X]\n",
    "    X = torch.tensor(X, dtype=torch.long, device=device)\n",
    "    y_true = torch.tensor(y_true, dtype=torch.long if USE_CLS else torch.float, device=device)\n",
    "    # step!\n",
    "    \n",
    "    #########\n",
    "#     X = torch.tensor([[-1, -1], [0, 0], [2, 2]], dtype=torch.float)\n",
    "#     y_true = torch.tensor([[-1.], [0.], [2.]], dtype=torch.float)\n",
    "    #########\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    y_preds = model(X)\n",
    "    if USE_CLS:\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(\n",
    "            weight=torch.tensor((0., 3., 3., 1., 1., 1.), device=device))\n",
    "#         print(y_preds.shape, y_true.shape)\n",
    "        loss = loss_fn(y_preds, y_true.reshape(-1,))\n",
    "    else:\n",
    "        loss = (y_preds - y_true) ** 2\n",
    "    \n",
    "    # update avg loss\n",
    "    loss_avg.insert(0, loss.mean().item())\n",
    "    loss_avg = loss_avg[:500]\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"It %d Avg loss: %.4f\" % (i, np.mean(loss_avg)))\n",
    "    if i and i % 10000 == 0:\n",
    "        mean_loss = compute_loss()\n",
    "        print(\"Mean loss: %.4f\" % mean_loss)\n",
    "    \n",
    "#     loss += _add_regularization(model) * 0.1\n",
    "    loss.mean().backward()\n",
    "    opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Don't use the classifier, it's not good\n",
    "# 2. With bs=32, clsweights 22111 adam and 100 neurons it works\n",
    "# Trained on 1e-4 to 100k, 1.035 valid. loss\n",
    "# Trained on 1e-5 to 150k, 1.025 valid. loss\n",
    "# Trained on 1e-5 to 200k, 1.025 valid. loss\n",
    "# 3. With clsweights 33111 it is the same up to 100k\n",
    "# 4. With 200 and 300 neurons (150 embedding) the same up to 100k\n",
    "# 5. With bottleneck of 50 neurons the same up to 100k\n",
    "## SCORE: 1.05, not good\n",
    "# Avg loss @100k: 0.95, @200k: 0.92\n",
    "\n",
    "# With everything, avg loss @300k: 0.87, but worse than before\n",
    "# Again just 4/5:\n",
    "# 0.98 and 1.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats: 3.847531561599506 3104.080220526129\n",
      "1.0454018920868178\n"
     ]
    }
   ],
   "source": [
    "mean_loss = compute_loss()\n",
    "print(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats: 3.8356630181218976 3120.383955674073\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def export_and_save(target, preds):\n",
    "    target_rows, target_cols = target.nonzero()\n",
    "    ids = [f\"r{row+1}_c{col+1}\" for row, col in zip(target_rows, target_cols)]\n",
    "    scores = [np.round(preds[row, col]) for row, col in zip(target_rows, target_cols)]\n",
    "    df = pd.DataFrame({\"Id\": ids, \"Prediction\": scores})\n",
    "    df.to_csv(\"preds.csv\", index=False, float_format='%.4f')\n",
    "\n",
    "export_and_save(X_test, get_predictions(X_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
