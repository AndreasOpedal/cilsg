{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Least Squares (Demo)\n",
    "This jupyter notebook is dedicated to demo one of our baseline models: Alternating Least Squares). You may need to install some python libraries if you run into import errors. Pip installation is provided in comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If use on ETHZ Euler cluster, you need to add '--user' option e.g. !pip install --user scikit-surprise\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise import AlgoBase, PredictionImpossible, Reader, Dataset, accuracy\n",
    "from surprise.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some helper functions\n",
    "def clean_df(df):\n",
    "    '''\n",
    "    Cleans initial representation to separate rows (users) and columns (movies) into columns with integer values\n",
    "    '''\n",
    "    row_str = df[\"Id\"].apply(lambda x: x.split(\"_\")[0])\n",
    "    row_id = row_str.apply(lambda x: int(x.split(\"r\")[1]) - 1)\n",
    "    col_str = df[\"Id\"].apply(lambda x: x.split(\"_\")[1])\n",
    "    col_id = col_str.apply(lambda x: int(x.split(\"c\")[1]) - 1)\n",
    "    \n",
    "    data_df = pd.DataFrame(data = {'row': row_id, 'col': col_id, 'Prediction': df.loc[:,'Prediction']})\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def read_submission_indexes(file_path):\n",
    "    '''\n",
    "    Reads the indexes for which to make the predictions.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (string): the file path to read\n",
    "\n",
    "    Returns:\n",
    "    indexes (list): a list where each element is a tuple of the form (row, column)\n",
    "    '''\n",
    "\n",
    "    data_train_raw = pd.read_csv(file_path).values\n",
    "\n",
    "    tuples = []\n",
    "\n",
    "    for i in range(data_train_raw.shape[0]):\n",
    "        indices, value = data_train_raw[i,0], int(data_train_raw[i,1])\n",
    "        indices = indices.split('_')\n",
    "        row, column = int(indices[0][1:])-1, int(indices[1][1:])-1 # indexing in the data starts from 1\n",
    "        tuples.append((row, column))\n",
    "\n",
    "    return tuples\n",
    "\n",
    "def write_predictions_to_csv(predictions, file_path):\n",
    "    '''\n",
    "    Writes on a csv file the predictions. The format of the prediction file is the following (second row is example):\n",
    "\n",
    "    Id, Prediction\n",
    "    r1_c1, 1\n",
    "\n",
    "    Parameters:\n",
    "    predictions (surprise.prediction_algorithms.predictions.Prediction): the object holding the predictions\n",
    "    file_path (string): the path to the prediction file\n",
    "    '''\n",
    "\n",
    "    # Define header\n",
    "    header = []\n",
    "    header.append('Id')\n",
    "    header.append('Prediction')\n",
    "\n",
    "    # Write file\n",
    "    with open(file_path, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        for row, col, pred in predictions:\n",
    "            # Row of the csv file\n",
    "            file_row = {}\n",
    "            # Build file row\n",
    "            file_row['Id'] = 'r'+ str(row+1) + '_c' + str(col+1)\n",
    "            file_row['Prediction'] = pred\n",
    "            # Write file row\n",
    "            writer.writerow(file_row)\n",
    "            \n",
    "def compute_rmse(target, pred):\n",
    "    mse = np.mean(np.square(target-pred))\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data_train_raw = pd.read_csv('../data/data-train.csv')\n",
    "# train dataset as data frame\n",
    "data_train_df = clean_df(data_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up surprise dataset\n",
    "reader = Reader()\n",
    "dataset = Dataset.load_from_df(data_train_df[['row', 'col', 'Prediction']], reader)\n",
    "\n",
    "# now set up training and test set, with a test split of 25%\n",
    "trainset, testset = train_test_split(dataset, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_column_norm_square(M):\n",
    "    \"\"\"\n",
    "    Calculate the sum of the squares of column vector norms\n",
    "\n",
    "    Param:\n",
    "    ========\n",
    "    M: np.array, input matrix\n",
    "    \"\"\"\n",
    "    return np.sum(np.square(np.linalg.norm(M, axis = 0)))\n",
    "\n",
    "def als(trainset, k, lamb, max_iter):\n",
    "    \"\"\"\n",
    "    Alternating Least Squares algorithm to decompose input trainset into the product of two lower rank matrices, i.e X_train = P^TQ, which P, Q are returned by this function.\n",
    "\n",
    "    Param:\n",
    "    ========\n",
    "    trainset: dataset formatted via surprise dataset\n",
    "    k: int, rank of lower representation in in vector space \n",
    "    lamb: float, regularized factor lambda\n",
    "    max_iter: int, number of maximum iterations\n",
    "\n",
    "    Return:\n",
    "    ========\n",
    "    P: np.array, low-rank matrix \n",
    "    Q: np.array, low-rank matrix\n",
    "    \"\"\"\n",
    "    m, n = trainset.n_users, trainset.n_items\n",
    "    # Get rating matrix\n",
    "    A = np.zeros((m, n))\n",
    "    for u, i, r in trainset.all_ratings():\n",
    "        A[u,i] = r\n",
    "    # Initialize P, Q\n",
    "    P = np.ones((k,m)) #user matrix\n",
    "    Q = np.ones((k,n)) #item matrix\n",
    "    Id_k = np.eye(N=k)\n",
    "    # Alternate to optimize\n",
    "    num_iter = 0\n",
    "    best_PQ = [None, None, None]\n",
    "    obs_idx = np.nonzero(A)\n",
    "    target = A[obs_idx]\n",
    "    num_rmse_incr = 0\n",
    "    while num_iter < max_iter:\n",
    "        # train rmse\n",
    "        pred =(P.T@Q)[obs_idx]\n",
    "        last_train_rmse = compute_rmse(target, pred)\n",
    "        # Optimize user matrix by fixing item matrix \n",
    "        for u in range(m):\n",
    "            obs_items = np.nonzero(A[u, :])\n",
    "            num_obs_items = obs_items[0].shape[0]\n",
    "            sum_qqT = np.sum([Q[:, i]*np.reshape(Q[:, i], (-1, 1)) for i in obs_items[0]], 0)\n",
    "            P[:, u] = np.squeeze(np.linalg.inv(sum_qqT+lamb*num_obs_items*Id_k) @ np.reshape(np.sum([A[u,item]*Q[:, item] for item in obs_items[0]], 0), (-1, 1)))\n",
    "        # Optimize item matrix by fixing user matrix\n",
    "        for i in range(n):\n",
    "            obs_users = np.nonzero(A[:, i])\n",
    "            num_obs_users = obs_users[0].shape[0]\n",
    "            sum_ppT = np.sum([P[:, u]*np.reshape(P[:, u], (-1, 1)) for u in obs_users[0]], 0)\n",
    "            Q[:, i] = np.squeeze(np.linalg.inv(sum_ppT+lamb*num_obs_users*Id_k) @ np.reshape(np.sum([A[u, i]*P[:, u] for u in obs_users[0]], 0), (-1, 1)))\n",
    "        best_PQ[0],  best_PQ[1], best_PQ[2] = best_PQ[1], best_PQ[2], (P, Q)\n",
    "        pred = (P.T@Q)[obs_idx]\n",
    "        train_rmse = compute_rmse(target, pred)        \n",
    "        if (num_iter+1) % 5 == 0:\n",
    "            print(\"Iter  {}: rmse error {}\".format(num_iter+1, train_rmse))\n",
    "        if train_rmse - last_train_rmse >= 0:\n",
    "            num_rmse_incr += 1\n",
    "        else:\n",
    "            num_rmse_incr =  0\n",
    "        if num_rmse_incr >=  2:\n",
    "            print(\"RMSE stops decreasing at iter {}, early stopping...\".format(num_iter-1))\n",
    "            return  best_PQ[0]\n",
    "        num_iter += 1\n",
    "    P, Q = best_PQ[2]\n",
    "    return P, Q\n",
    "\n",
    "class ALS(AlgoBase):\n",
    "    \"\"\"\n",
    "    Model via Alternating Least Square\n",
    "    \"\"\"\n",
    "    def __init__(self, k, lamb, max_iter):\n",
    "        self.k = k\n",
    "        self.lamb = lamb\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        start = time.time()\n",
    "        self.P, self.Q = als(self.trainset, self.k, self.lamb, self.max_iter)\n",
    "        print(\"Used time: {}\".format(time.time() - start))   \n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        return np.clip(np.dot(self.P[:, u], self.Q[:, i]), 1, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model via Grid Search\n",
    "Here we presented the best model via grid search on selected hyper-parameters. We stored the best hyper-parameters from experiments.\n",
    "\n",
    "Grid Search was run on Euler cluster with parallelization. We provide the commented code in the following section \"Grid Search\" only if you would like to re-run the search again. However, we do not recommend to do so without sufficient computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iter  5: rmse error 1.0188438604480456\nIter  10: rmse error 1.0029974188691189\nIter  15: rmse error 0.9983336284828854\nIter  20: rmse error 0.9964159854459105\nIter  25: rmse error 0.9928907078322519\nIter  30: rmse error 0.9711109889811866\nIter  35: rmse error 0.9560882997617441\nIter  40: rmse error 0.9422869316964702\nIter  45: rmse error 0.9224173758009514\nIter  50: rmse error 0.9063769574204679\nIter  55: rmse error 0.9031025960300871\nIter  60: rmse error 0.9026669211298688\nIter  65: rmse error 0.9024513082057595\nIter  70: rmse error 0.9023123896085158\nIter  75: rmse error 0.9022137441746727\nIter  80: rmse error 0.9021392963035701\nIter  85: rmse error 0.9020806743601565\nIter  90: rmse error 0.9020331362423839\nIter  95: rmse error 0.9019938403171276\nIter  100: rmse error 0.9019609910130367\nUsed time: 1756.3296949863434\nRMSE: 0.9905\n"
    }
   ],
   "source": [
    "# Best params\n",
    "k = 20\n",
    "lamb = 0.1\n",
    "max_iter = 100\n",
    "\n",
    "# Define the model\n",
    "model = ALS(k=k, lamb=lamb, max_iter=max_iter)\n",
    "\n",
    "# Fit the model with trainset\n",
    "model.fit(trainset)\n",
    "\n",
    "# Compute the predictions on testset\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Compute RMSE on testset\n",
    "test_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search \n",
    "The Grid Search was run to find our ALS model with lowest validation RMSE. You could modify the param_grid to experiment other ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid_search_model = GridSearchCV(ALS, param_grid = {\"k\":[3, 5, 10, 20], \"lamb\":[0.05, 0.1, 0.5, 1], max_iter\":[10, 30,50,100]}, n_jobs = -1)\n",
    "# grid_search_model.fit(dataset)\n",
    "# grid_search_model.best_score, model.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare submission indices\n",
    "pred_ids = read_submission_indexes(\"../data/sample-submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iter  5: rmse error 1.0203619791401577\nIter  10: rmse error 1.0045349402366655\nIter  15: rmse error 0.9998780378420568\nIter  20: rmse error 0.9979634542924309\nIter  25: rmse error 0.9969707985014635\nIter  30: rmse error 0.9813964653038831\nIter  35: rmse error 0.9754864713389988\nIter  40: rmse error 0.972235010942289\nIter  45: rmse error 0.9595719837489957\nIter  50: rmse error 0.9569756982040228\nIter  55: rmse error 0.9543549400211193\nIter  60: rmse error 0.9496515609419316\nIter  65: rmse error 0.9454709174511202\nIter  70: rmse error 0.9411906575531267\nIter  75: rmse error 0.937933394540405\nIter  80: rmse error 0.9348407487685714\nIter  85: rmse error 0.9328052335998263\nIter  90: rmse error 0.9319471856753584\nIter  95: rmse error 0.9318032229448242\nIter  100: rmse error 0.9317605055117049\nUsed time: 2174.0647082328796\n"
    }
   ],
   "source": [
    "# Best params\n",
    "model = ALS(k=k, lamb=lamb, max_iter=max_iter)\n",
    "# Fit on all training data\n",
    "all_train  = dataset.build_full_trainset()\n",
    "model.fit(all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on Kaggle test data# Predictions on Kaggle test data\n",
    "submission = []\n",
    "for r, c in pred_ids:\n",
    "    submission.append((r, c, model.estimate(r, c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "import csv\n",
    "# Save submission predictions\n",
    "submission_csv = \"../predictions/submission_als_{}.csv\".format(datetime.datetime.now().strftime('%Y%m%d_%H%M'))\n",
    "write_predictions_to_csv(submission, submission_csv)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}