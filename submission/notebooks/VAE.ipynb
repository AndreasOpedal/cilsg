{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "sys.path.append('../')\n",
    "import src.dataset\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = src.dataset.load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For final submission:\n",
    "train_on_full_dataset = False\n",
    "if train_on_full_dataset:\n",
    "    X_train = X_train + X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.count_nonzero(), X_valid.count_nonzero(), X_test.count_nonzero()\n",
    "valid_indices = list(set(zip(X_train.nonzero()[0], X_train.nonzero()[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_dim = 10000\n",
    "        self.item_dim = 1000\n",
    "        \n",
    "        self.layer_1 = nn.Linear(1000, 100)\n",
    "        self.layer_1a = nn.Linear(100, 100)\n",
    "        \n",
    "        self.vae = nn.Linear(100, 100)\n",
    "        \n",
    "        self.layer_3 = nn.Linear(50, 100)\n",
    "        self.cls_layer = nn.Linear(100, 1000)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        net_data = data\n",
    "\n",
    "        # Encoder layers\n",
    "        net_data = F.relu(self.layer_1(net_data))\n",
    "        net_data = F.dropout(net_data, training=self.training)\n",
    "        \n",
    "        net_data = F.relu(self.layer_1a(net_data))\n",
    "        net_data = F.dropout(net_data, training=self.training)\n",
    "        \n",
    "        # VAE bottleneck\n",
    "        vae = self.vae(net_data)\n",
    "        mus_q, log_sigmas_q = torch.split(vae, 50, dim=-1)\n",
    "        stds_q = torch.exp(0.5 * log_sigmas_q)\n",
    "        \n",
    "        KL = 0.5 * (-log_sigmas_q + torch.exp(log_sigmas_q) + mus_q ** 2 - 1)\n",
    "        KL = torch.sum(KL, dim=-1)\n",
    "        KL = torch.mean(KL)\n",
    "        \n",
    "        # Sample random value if training,\n",
    "        # use the mean during evaluation\n",
    "        if not self.training:\n",
    "            sampled_z = mus_q\n",
    "        else:\n",
    "            eps = torch.randn(stds_q.shape, dtype=torch.float, device=device)\n",
    "            sampled_z = mus_q + eps * stds_q\n",
    "        \n",
    "        # Decoder layers\n",
    "        net_data = F.relu(self.layer_3(sampled_z))\n",
    "        net_data = F.dropout(net_data, training=self.training)\n",
    "        \n",
    "        # Classification layer\n",
    "        y_score = self.cls_layer(net_data)\n",
    "        return y_score, KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "model = Autoencoder().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.count_nonzero(), X_valid.count_nonzero())\n",
    "\n",
    "def get_predictions(A):\n",
    "    \"\"\"Predict the full matrix using the autoencoder.\"\"\"\n",
    "    model.eval()\n",
    "    A_pred = np.zeros((10000, 1000))\n",
    "    valid_indices = list(set(zip(A.nonzero()[0], A.nonzero()[1])))\n",
    "    \n",
    "    # Split dataset in batches of 64 rows ...\n",
    "    for i in range(0, 10000, 64):\n",
    "        X = X_train[i:i+64].todense()\n",
    "        X_ = torch.tensor(X, dtype=torch.float, device=device)\n",
    "        y_preds, KL = model(X_)\n",
    "        \n",
    "        for i_ in range(y_preds.shape[0]):\n",
    "            x_i = y_preds[i_].detach().cpu()\n",
    "            A_pred[i+i_] = x_i\n",
    "    \n",
    "    return A_pred, valid_indices\n",
    "\n",
    "def compute_loss():\n",
    "    \"\"\"Compute the mean squared error between the ranking matrix and predictions.\"\"\"\n",
    "    A_pred, valid_indices = get_predictions(X_valid)\n",
    "    losses = np.square(X_valid - A_pred)\n",
    "    losses = [losses[i,j] for (i, j) in valid_indices]\n",
    "    mean_loss = np.mean(losses)\n",
    "    return mean_loss\n",
    "\n",
    "compute_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache positions of nonempty indices for training\n",
    "\n",
    "train_indices_perline = [[] for i_ in range(10000)]\n",
    "for i, j in valid_indices:\n",
    "    train_indices_perline[i].append((i, j))\n",
    "\n",
    "for line in train_indices_perline:\n",
    "    assert line\n",
    "assert len(train_indices_perline) == 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ANNEAL_ALPHA = 0.0\n",
    "NOISE_SCALE = 0.5\n",
    "MAX_STEPS = 500000\n",
    "\n",
    "loss_avg = []\n",
    "\n",
    "for i in range(0, MAX_STEPS):\n",
    "    # randomly sample 32 points from training matrix\n",
    "    model.train()\n",
    "    random_lines = random.sample(range(0, 10000), k=32)\n",
    "    X = X_train[random_lines].todense()\n",
    "    X = torch.tensor(X, dtype=torch.float, device=device)\n",
    "    y_true = X\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    # Important: Add random noise to the input to prevent overfitting\n",
    "    y_preds, KL = model(X + torch.randn(X.shape, device=device) * NOISE_SCALE)\n",
    "    \n",
    "    valid_mask = torch.zeros_like(y_preds, device=device)\n",
    "    for i_ in range(X.shape[0]):\n",
    "        i_real = random_lines[i_]\n",
    "        indices = [j for _,j in train_indices_perline[i_real]]\n",
    "        valid_mask[i_, indices] = 1.0\n",
    "\n",
    "    loss = (y_preds - y_true) ** 2\n",
    "    loss = loss[valid_mask != 0.]\n",
    "    neg_ELBO = loss + ANNEAL_ALPHA * KL\n",
    "    \n",
    "    # update avg loss\n",
    "    loss_avg.insert(0, loss.mean().item())\n",
    "    loss_avg = loss_avg[:1000]\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(\"It %d Avg loss: %.4f\" % (i, np.mean(loss_avg)))\n",
    "    if i and i % 1000 == 0:\n",
    "        mean_loss = compute_loss()\n",
    "        print(\"Mean loss: %.4f\" % mean_loss)\n",
    "    \n",
    "    neg_ELBO.mean().backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "    opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_loss = compute_loss()\n",
    "print(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predicted matrix can also be used as initialization for other models ...\n",
    "import pickle\n",
    "with open(\"array-for-svd.pkl\", \"wb\") as wp:\n",
    "    pickle.dump(get_predictions(X_train)[0], wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def export_and_save(target, preds):\n",
    "    target_rows, target_cols = target.nonzero()\n",
    "    ids = [f\"r{row+1}_c{col+1}\" for row, col in zip(target_rows, target_cols)]\n",
    "    scores = [preds[row, col] for row, col in zip(target_rows, target_cols)]\n",
    "    # Clip scores out of valid range\n",
    "    scores = [score if score <= 5.0 else 5.0 for score in scores]\n",
    "    scores = [score if score >= 1.0 else 1.0 for score in scores]\n",
    "    df = pd.DataFrame({\"Id\": ids, \"Prediction\": scores})\n",
    "    print(\"---Please check---\")\n",
    "    print(df.head())\n",
    "    df.to_csv(\"preds.csv\", index=False)\n",
    "\n",
    "export_and_save(X_test, get_predictions(X_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether we exported the array correctly ...\n",
    "import pickle\n",
    "with open(\"array-for-svd.pkl\", \"rb\") as fp:\n",
    "    arr = pickle.load(fp)\n",
    "    print(arr.shape, arr[0,7])\n",
    "    del arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
